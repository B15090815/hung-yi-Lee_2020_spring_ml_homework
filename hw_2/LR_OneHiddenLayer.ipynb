{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "from utils import sigmoid, get_batch, normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    a = np.maximum(0,z)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propogate(w,b,x):\n",
    "    a = x\n",
    "    for i in range(len(w)):\n",
    "        z = np.dot(w[i],a) + b[i]\n",
    "        a = sigmoid(z)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(w,b,x,y):\n",
    "    a = propogate(w,b,x)\n",
    "    predict = np.where(a > 0.5, 1., 0.)\n",
    "    accury = 1 - np.mean(np.abs(predict - y))\n",
    "    return accury"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of X is (510, 48896)\n",
      "the shape of Y is (1, 48896)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = np.load('data/train_x.npy')\n",
    "Y = np.load('data/train_y.npy')\n",
    "\n",
    "Xt = np.load('data/test_x.npy')\n",
    "Yt = np.load('data/test_y.npy')\n",
    "print('the shape of X is', X.shape)\n",
    "print('the shape of Y is', Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 500 batch,train set accuty is 0.797, loss is 0.492. Test Accury is 0.796\n",
      "Training on 1000 batch,train set accuty is 0.820, loss is 0.472. Test Accury is 0.805\n",
      "Training on 1500 batch,train set accuty is 0.820, loss is 0.435. Test Accury is 0.813\n",
      "Training on 2000 batch,train set accuty is 0.820, loss is 0.379. Test Accury is 0.810\n",
      "Training on 2500 batch,train set accuty is 0.812, loss is 0.401. Test Accury is 0.801\n",
      "Training on 3000 batch,train set accuty is 0.789, loss is 0.453. Test Accury is 0.798\n",
      "Training on 3500 batch,train set accuty is 0.805, loss is 0.408. Test Accury is 0.810\n",
      "Training on 4000 batch,train set accuty is 0.797, loss is 0.470. Test Accury is 0.804\n",
      "Training on 4500 batch,train set accuty is 0.797, loss is 0.382. Test Accury is 0.809\n",
      "Training on 5000 batch,train set accuty is 0.758, loss is 0.446. Test Accury is 0.807\n",
      "Training on 5500 batch,train set accuty is 0.844, loss is 0.382. Test Accury is 0.810\n",
      "Training on 6000 batch,train set accuty is 0.812, loss is 0.431. Test Accury is 0.810\n",
      "Training on 6500 batch,train set accuty is 0.820, loss is 0.400. Test Accury is 0.812\n",
      "Training on 7000 batch,train set accuty is 0.828, loss is 0.428. Test Accury is 0.811\n",
      "Training on 7500 batch,train set accuty is 0.812, loss is 0.388. Test Accury is 0.811\n",
      "Training on 8000 batch,train set accuty is 0.906, loss is 0.317. Test Accury is 0.806\n",
      "Training on 8500 batch,train set accuty is 0.805, loss is 0.411. Test Accury is 0.810\n",
      "Training on 9000 batch,train set accuty is 0.867, loss is 0.349. Test Accury is 0.810\n",
      "Training on 9500 batch,train set accuty is 0.820, loss is 0.389. Test Accury is 0.804\n",
      "Training on 10000 batch,train set accuty is 0.828, loss is 0.416. Test Accury is 0.809\n",
      "Training on 10500 batch,train set accuty is 0.789, loss is 0.438. Test Accury is 0.810\n",
      "Training on 11000 batch,train set accuty is 0.789, loss is 0.424. Test Accury is 0.808\n",
      "Training on 11500 batch,train set accuty is 0.812, loss is 0.412. Test Accury is 0.810\n",
      "Training on 12000 batch,train set accuty is 0.820, loss is 0.457. Test Accury is 0.810\n",
      "Training on 12500 batch,train set accuty is 0.812, loss is 0.422. Test Accury is 0.810\n",
      "Training on 13000 batch,train set accuty is 0.789, loss is 0.413. Test Accury is 0.810\n",
      "Training on 13500 batch,train set accuty is 0.820, loss is 0.389. Test Accury is 0.810\n",
      "Training on 14000 batch,train set accuty is 0.781, loss is 0.425. Test Accury is 0.809\n",
      "Training on 14500 batch,train set accuty is 0.859, loss is 0.360. Test Accury is 0.806\n",
      "Training on 15000 batch,train set accuty is 0.766, loss is 0.469. Test Accury is 0.810\n",
      "Training on 15500 batch,train set accuty is 0.758, loss is 0.405. Test Accury is 0.810\n",
      "Training on 16000 batch,train set accuty is 0.758, loss is 0.454. Test Accury is 0.811\n",
      "Training on 16500 batch,train set accuty is 0.734, loss is 0.510. Test Accury is 0.807\n",
      "Training on 17000 batch,train set accuty is 0.875, loss is 0.360. Test Accury is 0.805\n",
      "Training on 17500 batch,train set accuty is 0.805, loss is 0.410. Test Accury is 0.805\n",
      "Training on 18000 batch,train set accuty is 0.797, loss is 0.436. Test Accury is 0.806\n",
      "Training on 18500 batch,train set accuty is 0.805, loss is 0.395. Test Accury is 0.807\n",
      "Training on 19000 batch,train set accuty is 0.836, loss is 0.348. Test Accury is 0.812\n",
      "Training on 19500 batch,train set accuty is 0.836, loss is 0.408. Test Accury is 0.810\n",
      "Training on 20000 batch,train set accuty is 0.867, loss is 0.371. Test Accury is 0.807\n",
      "Training on 20500 batch,train set accuty is 0.820, loss is 0.383. Test Accury is 0.810\n",
      "Training on 21000 batch,train set accuty is 0.797, loss is 0.377. Test Accury is 0.809\n",
      "Training on 21500 batch,train set accuty is 0.773, loss is 0.467. Test Accury is 0.809\n",
      "Training on 22000 batch,train set accuty is 0.836, loss is 0.378. Test Accury is 0.809\n",
      "Training on 22500 batch,train set accuty is 0.820, loss is 0.431. Test Accury is 0.807\n",
      "Training on 23000 batch,train set accuty is 0.773, loss is 0.364. Test Accury is 0.807\n",
      "Training on 23500 batch,train set accuty is 0.859, loss is 0.345. Test Accury is 0.809\n",
      "Training on 24000 batch,train set accuty is 0.836, loss is 0.408. Test Accury is 0.809\n",
      "Training on 24500 batch,train set accuty is 0.789, loss is 0.437. Test Accury is 0.807\n",
      "Training on 25000 batch,train set accuty is 0.789, loss is 0.434. Test Accury is 0.809\n",
      "Training on 25500 batch,train set accuty is 0.781, loss is 0.430. Test Accury is 0.811\n",
      "Training on 26000 batch,train set accuty is 0.812, loss is 0.374. Test Accury is 0.807\n",
      "Training on 26500 batch,train set accuty is 0.836, loss is 0.402. Test Accury is 0.809\n",
      "Training on 27000 batch,train set accuty is 0.766, loss is 0.431. Test Accury is 0.809\n",
      "Training on 27500 batch,train set accuty is 0.789, loss is 0.423. Test Accury is 0.808\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-4b1802a06985>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0mda1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdz2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0mdz1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mda1\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0ma1\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0ma1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[0mdw1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdz1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m         \u001b[0mdb1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdz1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "epsilon = 1e-9\n",
    "TRAIN_STEP = 100\n",
    "lr = 0.25\n",
    "alpha = lr\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE_DECAY = 0.25\n",
    "BETA = 0.9\n",
    "\n",
    "INPUT_NODE = 510\n",
    "OUTPUT_NODE = 1\n",
    "HIDDEN_LAYER = 1020\n",
    "\n",
    "\n",
    "w1 = np.random.randn(HIDDEN_LAYER, INPUT_NODE) * 0.01\n",
    "b1 = np.zeros(shape=[HIDDEN_LAYER, 1])\n",
    "v_dw1 = np.zeros(shape=[HIDDEN_LAYER, INPUT_NODE])\n",
    "v_db1 = np.zeros(shape=[HIDDEN_LAYER, 1])\n",
    "\n",
    "w2 = np.random.randn(OUTPUT_NODE, HIDDEN_LAYER) * 0.01\n",
    "b2 = np.zeros(shape=[OUTPUT_NODE, 1])\n",
    "\n",
    "v_dw2 = np.zeros(shape=[OUTPUT_NODE, HIDDEN_LAYER])\n",
    "v_db2 = np.zeros(shape=[OUTPUT_NODE, 1])\n",
    "\n",
    "\n",
    "losses = []\n",
    "k = 1\n",
    "for epoch in range(TRAIN_STEP):\n",
    "    lr = alpha / (1 + LEARNING_RATE_DECAY * epoch)\n",
    "    index = 0\n",
    "    while True:\n",
    "        x, y, index = get_batch(index, BATCH_SIZE, X, Y)\n",
    "        if x is None:\n",
    "            break\n",
    "        \n",
    "        m = y.shape[1]\n",
    "        \n",
    "        z1 = np.dot(w1, x) + b1\n",
    "        a1 = sigmoid(z1)\n",
    "        \n",
    "        z2 = np.dot(w2, a1) + b2\n",
    "        a = sigmoid(z2)\n",
    "        \n",
    "        cross_entropy = (1 - y) * np.log(1 - a + epsilon) + y * np.log(a + epsilon)\n",
    "        loss = - np.mean(cross_entropy)\n",
    "        \n",
    "        dz2 = a - y\n",
    "        dw2 = np.dot(dz2, a1.T) / m\n",
    "        db2 = np.mean(dz2, axis=1, keepdims=True)\n",
    "        \n",
    "        da1 = np.dot(w2.T, dz2)\n",
    "        dz1 = da1 * a1 * (1-a1)\n",
    "        dw1 = np.dot(dz1, x.T) / m\n",
    "        db1 = np.mean(dz1, axis=1, keepdims=True)\n",
    "        \n",
    "        v_dw1 = BETA * v_dw1 + (1 - BETA) * dw1\n",
    "        v_db1 = BETA * v_db1 + (1 - BETA) * db1\n",
    "        v_dw2 = BETA * v_dw2 + (1 - BETA) * dw2\n",
    "        v_db2 = BETA * v_db2 + (1 - BETA) * db2\n",
    "        \n",
    "        w2 -= lr * v_dw2\n",
    "        b2 -= lr * v_db2\n",
    "        w1 -= lr * v_dw1\n",
    "        b1 -= lr * v_db1\n",
    "        \n",
    "        if k % 500 == 0:\n",
    "            losses.append(loss)\n",
    "            w = [w1,w2]\n",
    "            b = [b1,b2]\n",
    "            accury = test(w,b,Xt,Yt)\n",
    "            train_acc = test(w,b,x,y)\n",
    "            print('Training on %d batch,train set accuty is %.3f, loss is %.3f. Test Accury is %.3f'\\\n",
    "                  %(k,train_acc, loss, accury))\n",
    "        k += 1\n",
    "        \n",
    "    \n",
    "\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.title('batch loss')\n",
    "plt.xlabel('batch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test ='data/X_test'\n",
    "dft = pd.read_csv(path_test, dtype=np.float32)\n",
    "Xtest = dft.iloc[:, 1:].values\n",
    "Xtest = Xtest.T\n",
    "dft = None\n",
    "Xtest = normalization(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction shape is  (1, 27622)\n"
     ]
    }
   ],
   "source": [
    "# 使用训练模型预测数据\n",
    "weight = [w1,w2]\n",
    "bias = [b1,b2]\n",
    "a = propogate(weight, bias, Xtest)\n",
    "predict = np.where(a < 0.5, 0, 1)\n",
    "print('prediction shape is ', predict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(predict.T, columns=['label'])\n",
    "# num = 5\n",
    "path_result = 'data/result_{}_{}.csv'.format(HIDDEN_LAYER,lr)\n",
    "result.to_csv(path_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
